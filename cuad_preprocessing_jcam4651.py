# -*- coding: utf-8 -*-
"""CUAD_preprocessing_jcam4651.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/16PT87f_XYTIt1fh5lXmmOOYxa3K_Q_Gg

## Assignment 2: Data Preprocessing and Performance Tuning with Spark

Description: This script aims to demonstrate preprocessing techniques and performance tuning using python spark (PySpark) execution, using the data set from Atticus Project involving Contract Understanding Atticus Dataset (CUAD). The CUAD dataset are pre-processed and converted into proper format before entering the Natural Language Propreccing (NLP) task.

Input: This script accepts a file with json format with the data fields according to the CUAD data set such as title, paragraphs, context, qas, etc,. 

Output: The script outputs four output directories containing json files. Below are the four outputs:
Output 1 are the Positive samples
Output 2 are the Possible Negative samples
Output 3 are the Impossible Negative samples
Output 4 are the Unique Impossible Negative samples used to maximised coverage of test/training data.
"""

# Comment out -- only needed for Collab notebook
#!pip install pyspark

#from google.colab import files
#uploaded = files.upload()

#Connect to Spark Session
"""
spark.serializer - use when serialising RDDs to disk as well as shuffling data between worker nodes
spark.sql.shuffle.partitions - default 200, reduce to 10 to reduce number of partitions use when shuffling data for joins or aggregations
spark.sql.inMemoryColumnarStorage.compressed - set to True - for Spark to select a compression codec for each column based on data statistics
setLogLevel - set to "ERROR" to remove information and warning logs.
"""
from pyspark.sql import SparkSession
spark = SparkSession \
    .builder \
    .appName("CUAD_preprocessing") \
    .config("spark.serializer", "org.apache.spark.serializer.KryoSerializer") \
    .config("spark.sql.shuffle.partitions", 100) \
    .config("spark.sql.inMemoryColumnarStorage.compressed", True) \
    .getOrCreate() 
    
spark.sparkContext.setLogLevel("ERROR")

#Import Functions and Col - use in Spark Dataframe Transformations

from pyspark.sql import functions as F
from pyspark.sql.functions import col 
import json

#Define Functions

def get_answer(ans_s,ans_e,seq_s,seq_e):
  """get_answer function will evaluation the answer start and answer end positions based source sequence of 4096 bytes
    There are four conditional statements to account for every possible scenario of original answer_start, answer_end positional 
    values defined from the original (unsliced) context
    Scenario 1: Answer_start and Answer_end are in the sliced sequence
    Scenario 2: Answer_start is in the sliced sequence while Answer_end is NOT
    Scenario 3: Answer_end is in the sliced sequence while Answer_start is NOT
    Scenario 4: Answer_start and Answer_end are NOT in the sliced sequence
  """
  if (ans_s >= seq_s and ans_s < seq_e and ans_e <= seq_e):  #Scenario 1: Answer_start and Answer_end are in the sliced sequence
    ans_s_out = ans_s - seq_s
    ans_e_out = (ans_e - ans_s) + ans_s_out
  elif (ans_s >= seq_s and ans_s < seq_e and ans_e > seq_e): #Scenario 2: Answer_start is in the sliced sequence while Answer_end is not
    ans_s_out = ans_s - seq_s
    ans_e_out = 4096
  elif (ans_e > seq_s and ans_e <= seq_e and ans_s < seq_s): #Scenario 3: Answer_end is in the sliced sequence while Answer_start is not
    ans_s_out = 0
    ans_e_out = ans_e - seq_s
  else:                                                      #Scenario 4: Answer_start and Answer_end are NOT in the sliced sequence
    ans_s_out = 0
    ans_e_out = 0
  return ans_s_out, ans_e_out

def get_sequence(title,source,question,ans_start,ans_end,is_impossible):
  """get_sequence function will slice the original context to 4096 bytes with 2048 sliding window sequences
    The WHILE loop will continue to slice the original context until the starting byte becomes
    the biggest value (0,2048,4096,6144...) less than the length of the context 
    (e.g. length of context is 8000 ==> the biggest value of starting byte is 6144)
  """
  i=0
  seg_list=[]
  t = title
  q = question
  j = is_impossible
  if (ans_start == None): ans_start = 0                                       #Answer_start = 0 if value from original data is None or Null
  if (ans_end == None): ans_end = 0                                           #Answer_end = 0 if value from original data is None or Null
  while i < len(source):                                                      #The WHILE loop will continue to slice the original context until the starting byte becomes
    ans_s_out, ans_e_out = get_answer(ans_start,ans_end,i,i+4096)             #Call get_answer function
    seg = (title,source[i:i+4096],question,ans_s_out,ans_e_out,is_impossible)
    i=i+2048                                                                   #Sliding window of 2048 bytes
    if (seg != ''):
      seg_list.append(seg)
  return seg_list                                                               #Return 4096 slided sequences


def get_sequence_wmax(title,source,question,ans_start,ans_end,is_impossible,max):
  """get_sequence_wmax function is very much like get_sequence function except it adds a
  parameter for maximum sequence (max), which can be use when there is a cap or maximum number 
  of sequences needed to be generated on a given contract or question. 
  ==> Use when generating count of Negative samples per contract/question
  """
  i=0
  a=1
  seg_list=[]
  t = title
  q = question
  j = is_impossible
  if (ans_start == None): ans_start = 0
  if (ans_end == None): ans_end = 0
  max_seq = max
  while i < len(source):
    ans_s_out, ans_e_out = get_answer(ans_start,ans_end,i,i+4096)
    seg = (title,source[i:i+4096],question,ans_s_out,ans_e_out,is_impossible)
    i=i+2048
    if (seg != ''):
      seg_list.append(seg)
    if (a==max_seq): break
    a+=1
  return seg_list

def get_average(question,contracts):
  """get_average function gets average contract count per question
  ==> Use when generating max sequence count for impossible negative samples
  """
  sum = 0
  count = 0
  for i in contracts:
    try:
      sum = sum + count_ps_dict[i,question]
      count+= 1
    except KeyError:
      continue
  return (question,round(sum/count))

if __name__ == '__main__':
        
      #Get Top/Root/First Tier Columns
      """
      Unhash values based on dataset to be processed
      """
      top_df = spark.read.json("test.json")
      #top_df = spark.read.json("train_separate_questions.json")
      #top_df = spark.read.json("CUADv1.json")

      #Print Top Level Columns and Schema

      print("The Top Tier Columns:")
      top_df.show()
      print("The Schema of the Top Tier Columns:")
      top_df.printSchema()

      #Get Second Tier Columns
      """
      Use the Function explode or F.explode to extract the values inside data array
      Use Select to display the new columns
      <column_name>.* will show all elements inside the exploded data array
      Drop unnecessary columns
      """

      sec_df = top_df.withColumn('data_elements', F.explode('data')) \
      .select('version', 'data_elements.*') \
      .drop('data', 'version')

      #Print Second Tier Columns
      print("The Second Tier Columns:")
      sec_df.show()
      print("The Schema of the Second Tier Columns:")
      sec_df.printSchema()

      #Get Third Tier Columns
      """
      Use the Function explode or F.explode to extract the values inside data array
      Use Select to display the new columns
      <column_name>.* will show all elements inside the exploded data array
      Drop unnecessary columns
      """

      trd_df = sec_df.withColumn('paragraphs_elements', F.explode('paragraphs')) \
      .select('title','paragraphs_elements.*') \
      .drop('paragraphs')

      #Print Third Tier Columns
      print("The Third Tier Columns:")
      trd_df.show()
      print("The Schema of the Third Tier Columns:")
      trd_df.printSchema()

      #Get Fourth Tier Columns
      """
      Use the Function explode or F.explode to extract the values inside data array
      Use Select to display the new columns
      <column_name>.* will show all elements inside the exploded data array
      Drop unnecessary columns
      """

      frt_df = trd_df.withColumn('qas_elements', F.explode('qas')) \
      .select('title','context', 'qas_elements.*') \
      .drop('qas','id')

      #Print Fourth Tier Columns
      print("The Fourth Tier Columns:")
      frt_df.show()
      print("The Schema of the Fourth Tier Columns:")
      frt_df.printSchema()

      #Fifth Level Columns
      """
      Use the Function explode or F.explode to extract the values inside data array
      Use Select to display the new columns
      <column_name>.* will show all elements inside the exploded data array
      Drop unnecessary columns
      """

      fft_df = frt_df.withColumn('answer_elements', F.explode_outer('answers')) \
      .select('title','context', 'question', 'is_impossible', 'answer_elements.*') 

      #Print Fifth Tier Columns
      print("The Fifth Tier Columns:")
      frt_df.show()
      print("The Schema of the Fifth Tier Columns:")
      frt_df.printSchema()

      #Replace answers column with answer_start and answer_end
      """
      Schema gets flatten as elements on lower tiers are exploded one by one
      Get answer_end based on length of the answer text + answer_start value
      Select to display chosen columns
      Drop unnecessary columns
      """

      flat_df = fft_df.withColumn('len_of_answer', F.length('text')) \
      .withColumn('answer_end', col('answer_start')+col('len_of_answer')) \
      .select('title','context', 'question','answer_start','answer_end', 'is_impossible') \
      .drop('text')

      #Print Flattened Dataset
      print("The Flattened Dataset columns:")
      flat_df.show()
      print("The Schema of the Flattened Dataset is")
      flat_df.printSchema()

      #Convert the flattened dataframe to RDD to map dataset with custom functions later in the script
      """
      The dataframe will be converted to RDD primarily to use custom functions needed to split the context values
      into 4096 bytes with 2048 sliding window
      Use tuple function to remove unnecessary headers when converting dataframes to RDD
      """
      start_rdd = flat_df.rdd.map(lambda x: tuple(x)) \
      .persist()

      #Print the start RDD converted from the flattened dataframe
      print("start_RDD ==> RDD converted from the flattened dataframe [first 5 elements]")
      print("\ndataset format: (title, context, question, answer_start, answer_end, is_impossible)\n")
      x = start_rdd.take(5)
      for i in x:
        print(i)
      print("\nThe number of elements or rows in start_RDD:",start_rdd.count())

      #Get POSITIVE SAMPLES
      """ Get positive samples
      1) Start by filtering all sequences with value of is_impossible == False
      2) Map values with the custom function get_sequence to get only 4096 bytes of sequences
      3) FlatMap to flatten the returned list
      4) Filter the sequences with answer_start and answer_end NOT equal to 0
      """
      positive_rdd = start_rdd.filter(lambda x: x[5] == False) \
      .map(lambda x: get_sequence(x[0],x[1],x[2],x[3],x[4],x[5])) \
      .flatMap(lambda x: x) \
      .filter(lambda x: x[3] != 0 and x[4]!= 0) \
      .persist()

      #Print the positive samples rdd
      print("positive_RDD ==> RDD with only positive samples [first 5 elements]")
      print("\ndataset format: (title, context[4096 bytes], question, answer_start, answer_end, is_impossible)\n")
      x = positive_rdd.take(5)
      for i in x:
        print(i)
      print("\nThe number of elements or rows in positive_RDD:",positive_rdd.count())

      #Get POSSIBLE NEGATIVE samples
      """
      1) Get count of positive samples per contract, per question/category using built-in RDD functions
      2) CollectAsMap to store as dictionary type variable 
      3) Get possible negative samples by using map and custom function get_sequence_wmax
      """

      #1
      count_ps_rdd= positive_rdd.map(lambda x: (x[0], x[2])) \
      .map(lambda x: (x, 1)) \
      .reduceByKey(lambda x,y: x + y) \
      .sortByKey() \
      .cache()

      #2
      count_ps_dict = count_ps_rdd.collectAsMap()
      count_ps_key_list = count_ps_rdd.map(lambda x: x[0])\
      .collect()

      #Print the positive samples rdd
      print("count_ps_rdd ==> count of all positive samples per contract and per question [first 5 elements]")
      print("\ndataset format: (title, question, count)\n")
      x = count_ps_rdd.take(5)
      for i in x:
        print(i)
      print("\nThe number of elements or rows in count_ps_RDD:",count_ps_rdd.count())

      #3
      possible_neg_rdd = start_rdd.filter(lambda x: (x[0],x[2]) in count_ps_key_list) \
      .map(lambda x: get_sequence_wmax(x[0],x[1],x[2],x[3],x[4],x[5],count_ps_dict[x[0],x[2]])) \
      .flatMap(lambda x: x) \
      .filter(lambda x: x[3] == 0 and x[4] == 0) \
      .distinct() \
      .persist()

      #Print the positive samples rdd
      print("\n\npossible_neg_rdd ==> RDD with only POSSIBLE negative samples [first 5 elements]")
      print("\ndataset format: (title, context[4096 bytes], question, answer_start, answer_end, is_impossible)\n")
      x = possible_neg_rdd.take(5)
      for i in x:
        print(i)
      print("\nThe number of elements or rows in possible_neg_ps_RDD:",possible_neg_rdd.count())

      #Get IMPOSSIBLE NEGATIVE samples
      """
      1) Get average number of positive samples per question for other contracts
          using built in rdd transformations
      2) CollectAsMap to store as dictionary type variable 
      3) Get impossible negative samples by using map and custom function get_sequence_wmax
      """
      #list of contract titles ==> use in get_average function
      contracts_list = positive_rdd.map(lambda x: x[0]) \
      .distinct() \
      .collect()

      #questions RDD ==> use for transformation to get average
      questions_rdd = positive_rdd.map(lambda x: x[2]) \
      .distinct()

      #questions list ==> use in get_average funcyopm
      questions_list = questions_rdd.collect()

      #1
      questions_ave_rdd = questions_rdd.map(lambda x: get_average(x,contracts_list))
      #2
      questions_ave_dict = questions_ave_rdd.collectAsMap()
      #3
      impossible_neg_rdd = start_rdd.filter(lambda x: x[5] == True) \
      .filter(lambda x: x[2] in questions_list) \
      .map(lambda x: get_sequence_wmax(x[0],x[1],x[2],x[3],x[4],x[5],questions_ave_dict[x[2]])) \
      .flatMap(lambda x: x) \
      .distinct() \
      .persist()

      #Print the question average rdd
      print("questions_ave_rdd ==> average number of positive samples per question for other contracts [first 5 elements]")
      print("\ndataset format: (question, average)\n")
      x = questions_ave_rdd.take(5)
      for i in x:
        print(i)
      print("\nThe number of elements or rows in questions_ave_RDD:",questions_ave_rdd.count())

      #Print the impossible negative samples rdd
      print("\n\nimpossible_neg_rdd ==> RDD with only IMPOSSIBLE negative samples [first 5 elements]")
      print("\ndataset format: (title, context[4096 bytes], question, answer_start, answer_end, is_impossible)\n")
      x = impossible_neg_rdd.take(5)
      for i in x:
        print(i)
      print("\nThe number of elements or rows in impossible_neg_RDD:",impossible_neg_rdd.count())

      #Validate Maximum Coverage of generated samples by checking uniqueness of samples
      """
      1) Combine all positive and negative sequence to get count of unique sequences
      2) Get maximum count of unique sequence from original dataset and compare with the generated samples
      3) Extract the negative samples that will generate maximum coverage
      """

      #1
      pos_uniq_rdd = positive_rdd.map(lambda x: (x[1])) \
      .distinct()

      pos_neg_uniq_rdd = possible_neg_rdd.map(lambda x: (x[1])) \
      .distinct()

      imp_neg_uniq_rdd = impossible_neg_rdd.map(lambda x: (x[1])) \
      .distinct()

      combined_uniq_list = pos_uniq_rdd.union(pos_neg_uniq_rdd) \
      .union(imp_neg_uniq_rdd) \
      .distinct() \
      .collect()
      
      #2a
      original_uniq_rdd = start_rdd.map(lambda x: get_sequence(x[0],x[1],x[2],x[3],x[4],x[5])) \
      .flatMap(lambda x: x) \
      .map(lambda x: (x[1])) \
      .distinct()

      #2b - Compare
      seq_gap = original_uniq_rdd.filter(lambda x: x not in combined_uniq_list)
      seq_gap_list = seq_gap.collect()
      seq_gap.take(10)

      #There will be x more samples generated if we aim for maximum sample coverage
      #3
      impossible_neg_plus_rdd = start_rdd.filter(lambda x: x[5] == True) \
      .map(lambda x: get_sequence(x[0],x[1],x[2],x[3],x[4],x[5])) \
      .flatMap(lambda x: x) \
      .filter(lambda x: x[1] in seq_gap_list) \
      .distinct() \
      .persist()

      #Print the imposible negative samples rdd that will have maximum coverage of training dataset
      print("\n\nimpossible_neg_plus_rdd ==> RDD with only IMPOSSIBLE negative samples that will have maximum coverage of training data [first 5 elements]")
      print("\ndataset format: (title, context[4096 bytes], question, answer_start, answer_end, is_impossible)\n")
      x = impossible_neg_plus_rdd.take(5)
      for i in x:
        print(i)
      print("\nThe number of elements or rows in impossible_neg_plus_RDD:",impossible_neg_plus_rdd.count())

      #Format the sample sequences for json file output
      """
      formatting the sample sequences to json files output by converting the RDD back to dataframe 
      and using a defined schema
      """

      from pyspark.sql.types import StructType,StructField,StringType,IntegerType,ArrayType

      sample_Schema = StructType([      
                StructField('source', StringType(), True),
                StructField('question', StringType(), True),
                StructField('answer_start', IntegerType(), True),
                StructField('answer_end', IntegerType(), True),
        ])

      #Eliminate unnecessary elements
      positive_out_rdd = positive_rdd.map(lambda x: (x[1],x[2],x[3],x[4]))
      possible_neg_out_rdd = possible_neg_rdd.map(lambda x: (x[1],x[2],x[3],x[4]))
      impossible_neg_out_rdd = impossible_neg_rdd.map(lambda x: (x[1],x[2],x[3],x[4]))
      impossible_neg_plus_out_rdd = impossible_neg_plus_rdd.map(lambda x: (x[1],x[2],x[3],x[4]))

      #Covert to dataframe
      positive_out_df = positive_out_rdd.toDF(sample_Schema).sort('source','question')
      possible_neg_out_df = possible_neg_out_rdd.toDF(sample_Schema).sort('source','question')
      impossible_neg_out_df = impossible_neg_out_rdd.toDF(sample_Schema).sort('source','question')
      impossible_neg_plus_out_df = impossible_neg_plus_out_rdd.toDF(sample_Schema).sort('source','question')

      #Print output schema
      print("The output schema is")
      positive_out_df.printSchema()
      positive_out_df.show()

      # Output Json files
      positive_out_df.write.json('positives')
      print("output positive samples")
      possible_neg_out_df.write.json('possible_negatives')
      print("output possible negative samples")
      impossible_neg_out_df.write.json('impossible_negatives')
      print("output impossible_negative samples")
      impossible_neg_plus_out_df.write.json('impossible_negatives_plus')
      print("output additional impossible negative for max coverage")

      #Delete folders to repeat testing
      #%rm -rf 'positives'
      #%rm -rf 'possible_negatives'
      #%rm -rf 'impossible_negatives'
      #%rm -rf 'impossible_negatives_plus'